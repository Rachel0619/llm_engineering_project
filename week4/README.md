## Notes

### Comparing the following features of LLMs

- Open-source or closed
- Release date and knowledge cut-off
- Parameters
- Training tokens
- Context length

### Comparing the following costs of LLMs

- Inference cost (API charge, subscription, runtime compute)
- Training cost
- Build cost
- Time to Market
- Rate limits
- Speed
- Latency
- License

### The Chinchilla Scaling Law

Number of parameters is proportional to the number of training tokens

### Benchmarks for Evaluating LLMs

| **Benchmark** | **What's being evaluated**      | **Description**                                                                 |
|---------------|---------------------------------|---------------------------------------------------------------------------------|
| ARC           | Reasoning                      | A benchmark for evaluating scientific reasoning; multiple-choice questions      |
| DROP          | Language Comp                  | Distill details from text, then add, count, or sort                             |
| HellaSwag     | Common Sense                   | "Harder Endings, Long Contexts, and Low Shot Activities"                        |
| MMLU          | Understanding                  | Factual recall, reasoning, and problem solving across 57 subjects              |
| TruthfulQA    | Accuracy                       | Robustness in providing truthful replies in adversarial conditions             |
| Winogrande    | Context                        | Test the LLM's understanding of context and resolution of ambiguity            |
| GSM8K         | Math                           | Math and word problems taught in elementary and middle schools                 |


